{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLyyCvCazrEJn9yKdi01qw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krissme28/-studying_ML/blob/main/1_%D0%B7%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lnbdrtDBpReq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем набор данных о ценах на жилье в Калифорнии https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n"
      ],
      "metadata": {
        "id": "Byka_0Hl6_0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Загрузка набора данных California housing\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target"
      ],
      "metadata": {
        "id": "07ZdUPQYpSpP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В наборе данных California housing входные данные представляют собой матрицу признаков, где каждая строка соответствует отдельному образцу (например, дому), а каждый столбец представляет собой отдельный признак (например, медианное значение дохода, количество комнат и т.д.)"
      ],
      "metadata": {
        "id": "j1bqjTRjCnJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLEKMUsANf8",
        "outputId": "f1b89bc0-3ad6-45d9-fcb3-c9f5f1cf7031"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
            "\n",
            "   Longitude  \n",
            "0    -122.23  \n",
            "1    -122.22  \n",
            "2    -122.24  \n",
            "3    -122.25  \n",
            "4    -122.25  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Нормализация данных\n",
        "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "y = (y - y.min()) / (y.max() - y.min())"
      ],
      "metadata": {
        "id": "dvOiyHe_qCkr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем последовательную модель нейронной сети с двумя слоями Dense. Первый слой имеет 16 нейронов и функцию активации ReLU, а второй слой имеет 1 нейрон и функцию активации linear. Модель компилируется с оптимизатором 'adam' и функцией потерь 'mean_squared_error', затем обучается на данных."
      ],
      "metadata": {
        "id": "5ESRGYBA8nDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Разделение на обучающий и тестовый наборы данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Создание и обучение нейронной сети\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)"
      ],
      "metadata": {
        "id": "2UlYSlf9qETy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c8f5ec-ea5a-488b-a339-539ecc48168c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "465/465 [==============================] - 2s 2ms/step - loss: 0.1648 - val_loss: 0.0356\n",
            "Epoch 2/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0272 - val_loss: 0.0258\n",
            "Epoch 3/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0234 - val_loss: 0.0249\n",
            "Epoch 4/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0230 - val_loss: 0.0245\n",
            "Epoch 5/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0226 - val_loss: 0.0241\n",
            "Epoch 6/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0223 - val_loss: 0.0238\n",
            "Epoch 7/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0221 - val_loss: 0.0235\n",
            "Epoch 8/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0218 - val_loss: 0.0234\n",
            "Epoch 9/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0216 - val_loss: 0.0235\n",
            "Epoch 10/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0215 - val_loss: 0.0232\n",
            "Epoch 11/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0213 - val_loss: 0.0229\n",
            "Epoch 12/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0212 - val_loss: 0.0227\n",
            "Epoch 13/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0211 - val_loss: 0.0228\n",
            "Epoch 14/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0209 - val_loss: 0.0226\n",
            "Epoch 15/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0236\n",
            "Epoch 16/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0225\n",
            "Epoch 17/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.0223\n",
            "Epoch 18/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0223\n",
            "Epoch 19/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.0225\n",
            "Epoch 20/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0205 - val_loss: 0.0222\n",
            "Epoch 21/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0204 - val_loss: 0.0220\n",
            "Epoch 22/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0204 - val_loss: 0.0221\n",
            "Epoch 23/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0203 - val_loss: 0.0231\n",
            "Epoch 24/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0202 - val_loss: 0.0220\n",
            "Epoch 25/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0201 - val_loss: 0.0218\n",
            "Epoch 26/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0201 - val_loss: 0.0220\n",
            "Epoch 27/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0200 - val_loss: 0.0222\n",
            "Epoch 28/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0199 - val_loss: 0.0220\n",
            "Epoch 29/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0199 - val_loss: 0.0219\n",
            "Epoch 30/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0198 - val_loss: 0.0217\n",
            "Epoch 31/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0197 - val_loss: 0.0218\n",
            "Epoch 32/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0196 - val_loss: 0.0214\n",
            "Epoch 33/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0195 - val_loss: 0.0214\n",
            "Epoch 34/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0195 - val_loss: 0.0217\n",
            "Epoch 35/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0194 - val_loss: 0.0216\n",
            "Epoch 36/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0193 - val_loss: 0.0218\n",
            "Epoch 37/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0193 - val_loss: 0.0212\n",
            "Epoch 38/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0192 - val_loss: 0.0210\n",
            "Epoch 39/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0192 - val_loss: 0.0221\n",
            "Epoch 40/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0192 - val_loss: 0.0209\n",
            "Epoch 41/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0191 - val_loss: 0.0214\n",
            "Epoch 42/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0192 - val_loss: 0.0209\n",
            "Epoch 43/50\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.0191 - val_loss: 0.0207\n",
            "Epoch 44/50\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.0189 - val_loss: 0.0208\n",
            "Epoch 45/50\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.0187 - val_loss: 0.0207\n",
            "Epoch 46/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0188 - val_loss: 0.0206\n",
            "Epoch 47/50\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.0187 - val_loss: 0.0206\n",
            "Epoch 48/50\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.0187 - val_loss: 0.0209\n",
            "Epoch 49/50\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 0.0187 - val_loss: 0.0205\n",
            "Epoch 50/50\n",
            "465/465 [==============================] - 1s 2ms/step - loss: 0.0186 - val_loss: 0.0205\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7cade435b7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `X_clean` и `y_clean`: Это обучающие данные без отравления (чистые данные). `X_clean` представляет собой матрицу признаков (входных данных), а `y_clean` - соответствующие метки классов.\n",
        "\n",
        "2. `percent_poison`: Это процент отравления, который указывает, какую долю данных нужно \"отравить\" (изменить) для создания атаки.\n",
        "\n",
        "3. `target_value`: Значение, которое устанавливаем для \"отравленных\" меток классов. (0.5)\n",
        "\n",
        "4. `n_clean`: Количество чистых образцов в исходных данных.\n",
        "\n",
        "5. `num_poison`: Количество образцов, которые нужно \"отравить\" в соответствии с заданным процентом отравления. Вычисляется как округленное значение отношения процента отравления к (1 - процент отравления).\n",
        "\n",
        "6. `poison_indices`: Генерация случайных неповторяющихся индексов образцов для отравления.\n",
        "\n",
        "7. Создание копий отравляющих образцов: Создание копии чистых данных с индексами из `poison_indices`.\n",
        "\n",
        "8. Задание отравляющего значения: Метки классов в отравленных образцах (`y_poison`) устанавливаются в `target_value`.\n",
        "\n",
        "9. Добавление отравленных образцов: Отравленные признаки и метки классов объединяются с чистыми данными для создания нового обучающего набора с \"отравленными\" образцами.\n",
        "\n",
        "10. Возвращение отравленных данных: Функция возвращает новые \"отравленные\" образцы (`X_backdoor` и `y_backdoor`). Текст, выделенный полужирным шрифтом"
      ],
      "metadata": {
        "id": "fHwcYBm_-1fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Функция для проведения атаки отравления\n",
        "def generate_backdoor(X_clean, y_clean, percent_poison, target_value):\n",
        "    n_clean = X_clean.shape[0]\n",
        "    num_poison = round((percent_poison * n_clean) / (1 - percent_poison))\n",
        "\n",
        "    # Выбираем случайные индексы для отравления\n",
        "    poison_indices = np.random.choice(n_clean, num_poison, replace=False)\n",
        "\n",
        "    # Создаем копии отравляющих образцов\n",
        "    X_poison = X_clean[poison_indices].copy()\n",
        "    y_poison = y_clean[poison_indices].copy()\n",
        "\n",
        "    # Задаем отравляющее значение\n",
        "    y_poison[:] = target_value\n",
        "\n",
        "    # Добавляем отравленные образцы к исходному обучающему набору\n",
        "    X_backdoor = np.concatenate((X_clean, X_poison), axis=0)\n",
        "    y_backdoor = np.concatenate((y_clean, y_poison), axis=0)\n",
        "\n",
        "    return X_backdoor, y_backdoor"
      ],
      "metadata": {
        "id": "gXVOesvkqGMm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование меток в one-hot encoding\n",
        "y_test_encoded = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "# Оцениваем производительность модели на тестовых данных до атаки\n",
        "y_pred_before_attack = model.predict(X_test)\n",
        "accuracy_before_attack = np.mean(np.argmax(y_pred_before_attack, axis=1) == np.argmax(y_test_encoded, axis=1))\n",
        "print(\"Точность на тестовых данных до атаки: %.2f%%\" % (accuracy_before_attack * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TmG4Xw2ry0K",
        "outputId": "68fa5c4b-59be-44dd-baea-a38787dedf42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - 0s 2ms/step\n",
            "Точность на тестовых данных до атаки: 95.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вызываем функцию атаки отравления с процентом отравления 10% и целевым значением 0.5\n",
        "percent_poison = 0.5\n",
        "target_value = 0.5\n",
        "X_train_poisoned, y_train_poisoned = generate_backdoor(X_train, y_train, percent_poison, target_value)\n",
        "\n",
        "# Обучаем модель на отравленных данных\n",
        "model.fit(X_train_poisoned, y_train_poisoned, epochs=50, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWP8FUkiqakr",
        "outputId": "1f5b4c05-197a-489e-94ae-8ae8b49b11b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0221\n",
            "Epoch 2/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0217\n",
            "Epoch 3/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0217\n",
            "Epoch 4/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0216\n",
            "Epoch 5/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0216\n",
            "Epoch 6/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0216\n",
            "Epoch 7/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0216\n",
            "Epoch 8/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0216\n",
            "Epoch 9/50\n",
            "1032/1032 [==============================] - 2s 1ms/step - loss: 0.0215\n",
            "Epoch 10/50\n",
            "1032/1032 [==============================] - 2s 1ms/step - loss: 0.0216\n",
            "Epoch 11/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0215\n",
            "Epoch 12/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 13/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0215\n",
            "Epoch 14/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0215\n",
            "Epoch 15/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0215\n",
            "Epoch 16/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 17/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 18/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 19/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 20/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 21/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 22/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 23/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0214\n",
            "Epoch 24/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 25/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 26/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 27/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 28/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 29/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 30/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 31/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 32/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 33/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 34/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 35/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 36/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0213\n",
            "Epoch 37/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 38/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 39/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 40/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 41/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 42/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 43/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 44/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 45/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 46/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 47/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 48/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0211\n",
            "Epoch 49/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0212\n",
            "Epoch 50/50\n",
            "1032/1032 [==============================] - 2s 2ms/step - loss: 0.0211\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7cade41ff790>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы наблюдаем падение точности и увеличение среднеквадратической ошибки (MSE) на тестовых данных после проведения атаки отравления. Это происходит из-за введения отравленных данных в обучающий набор, что приводит к изменению обучающего процесса и поведения модели."
      ],
      "metadata": {
        "id": "vKmoa9uMu3oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Оцениваем производительность модели на тестовых данных с отравлением\n",
        "mse_poisoned = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"MSE на тестовых данных с отравлением: %.4f\" % mse_poisoned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bre5WGIHqlXG",
        "outputId": "1416bf5a-81ea-4018-c69a-0ca56168940e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE на тестовых данных с отравлением: 0.0285\n"
          ]
        }
      ]
    }
  ]
}